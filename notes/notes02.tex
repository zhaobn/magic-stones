% Created: 2019-10-30
% Second draft of normative models for the magic stones task
% http://github.com/zhaobn/magic_stones
% Main changes: 

\documentclass{article}
\title{
	[Magic Stones] Normative Models Draft \\
	\large Working Title:
	Theory formation in dynamic causal generalization
	%Old: Category Bias in Dynamic Causal Generalization
}
\author{
	Bonan Zhao\\
	b.zhao@ed.ac.uk
}


% Text formats: margin, font, spacing
\usepackage[margin=0.8in]{geometry}
\usepackage{charter}
\renewcommand{\baselinestretch}{1.3}

% Shorthands
\newcommand{\featurespace}{F}
\newcommand{\colorspace}{\mathbb{CL}}
\newcommand{\shapespace}{\mathbb{SP}}

% Math stuff
\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Settings for drawings
\usepackage{tikz,wrapfig}
\usetikzlibrary{shapes.geometric, arrows}

\begin{document}
\maketitle

\section{Models}

\subsection*{Task formalization}

A magic stone $g$ is formalized as an expression $g = c_g \wedge s_g$, 
where its color $c_g$ takes value from a finite color space $\colorspace=\{red, yellow, blue\}$, 
and its shape $s_g$ takes value from a finite shape space $\shapespace=\{round, circle, triangle\}$.
In total, the set of all magic stones $G$ is of size $(|\colorspace| \times |\shapespace|) = 9$.

% Figure of a task formalization
\tikzstyle{node} = [circle, minimum width=0.8cm, minimum height=0.8cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]
\begin{wrapfigure}{l}{0.4\textwidth}
\begin{center}
	\begin{tikzpicture}
		\node (a) [node] {$A$};
		\node (c) [node, right of=a, xshift=0.8cm, fill=gray!30] {$C$};
		\node (r) [node, right of=c, xshift=0.8cm] {$R$};
		\node (rp) [node, below right of=c, yshift=-0.8cm, xshift=0.2cm] {$R'$};
		\draw [arrow] (a) -- (c);
		\draw [arrow] (c) -- (rp);
		\draw [arrow] (r) -- (rp);
	\end{tikzpicture}
	\caption{Model of a magic stone task.}
	\label{model}
\end{center}
\end{wrapfigure}

A magic stone task consists of an agent stone $A$ that changes the recipient stone $R$ into $R'$.
An ordered set of $(A, R, R')$ forms a data point in this model. There are $|G|^3 = 729$ outcomes in the entire sample space.

$R'$ is conditional on its previous state $R$, and the causal power of agent stone $A$. As shown in Figure~\ref{model}, agent stone $A$ is observable, but its causal power $C$ is latent (shaded in gray).
The joint distribution of $P(A, R, R') = P(R'|R)P(R)P(R'|C)P(C|A)P(A)$\footnote{
	Lazy notation P(A) stands for P(A=a).
}.
In practice, the task is to infer the changed stone $R'$ given $A$ and $R$, which yields $P(R'|\theta) = P(R'|R)P(R'|C)P(C|A)$.

\subsection*{Causal power}

Causal power $C$ is a function $f(A, R) = R'$, stating that given an agent stone $A$ and recipient $R$, $R$ changes into $R'$. In other words,

\begin{equation}\label{pdf}
	P(R'|A, R, C) =
	\begin{cases}
		1 &\text{if } f_C(A, R) = R'\\
		0 &\text{otherwise}
	\end{cases}
\end{equation}

To better capture the uncertainty of real life, we use a softmax over Equation~(\ref{pdf}) to smooth out the cutoffs. Given $A, R$, there are 9 possible $R'$s, hence set $b=e^9$ as the base for this specific softmax, yielding

\begin{equation}\label{spdf}
	P(R'|A, R, C) =
	\begin{cases}
		.992 &\text{if } f_C(A, R) = R'\\
		.001 &\text{otherwise}
	\end{cases}
\end{equation}

\subsection*{Bayesian update}

In a standard Bayesian context, causal power functions form the hypothesis space $H$. 

During the \emph{learning} phase, assuming a uniform prior distribution $P(h) = \frac{1}{|H|}$, and likelihood $P(D|h)$ given by Equation~(\ref{spdf}), the posterior distribution of causal power functions given data is calculated by:

\begin{equation}\label{learning}
	P(h|D) = \frac{P(D|h)P(h)}{\sum_{h_i \in H}P(D|h_i)P(h_i)}
\end{equation}	


While \emph{generalizing}, the posterior predictive distribution is given by marginalizing the distribution of new data $\tilde{d}$ over the posterior distribution ($Z$ is the normalizing factor):

\begin{equation}\label{gen}
	P(\tilde{d}|D) = Z \sum_{h\in H}P(\tilde{d}|h)P(h|D)
\end{equation}


\subsection*{Hierarchical Bayesian models}

% Figure for hierarchical Bayesian model framework
\tikzstyle{node} = [ellipse, minimum width=1.62cm, minimum height=0.8cm, text centered, draw=black]
\tikzstyle{box} = [rectangle, minimum width=2.4cm, minimum height=3.4cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]
\begin{wrapfigure}{r}{0.3\textwidth}
\begin{center}
	\begin{tikzpicture}
		\node (ts) [node] {$T$};
		\node (hs) [node, below of=ts, yshift=-0.8cm] {$H$};
		\node (d) [node, below of=hs, yshift=-0.8cm] {$D$};
		\node (b) [box, below of=ts, yshift=-1.6cm] {};
		\draw [arrow] (ts) -- (hs);
		\draw [arrow] (hs) -- (d);
	\end{tikzpicture}
	\caption{A hierarchical Bayesian model.}
	\label{hbm}
\end{center}
\end{wrapfigure}

What causal power functions people may inspect forms the key part of this model. We follow a hierarchical Bayesian model (HBM) framework to tackle this problem.

The hierarchical Bayesian model we consider here consists of three levels of abstraction: theories, hypotheses, and data. As illustrated in Figure~\ref{hbm}, at the highest level of abstraction is theory space $T$, and each theory $t \in T$ generates some causal power functions, forming its hypothesis space $H$. Hypothesis generates data $D$, at the lowest level of abstraction in this framework.

Previous section has defined how to update posterior beliefs about the hypothesis space, and in this section we consider some possible theories that generate different hypothesis spaces.

\subsubsection*{Candidate theories}

A causal power function deals with cause conditions, and defines what effects will take place.

From the cause conditions perspective, a cause condition can be either a specific object, or a specific feature. An object-based cause condition is something like $A \in \{s_1\}$


\section{Model predictions}

\section{Alternatives}








\end{document}