% Created: 2019-10-27
% First draft of normative models for the magic stones task
% http://github.com/zhaobn/magic_stones

\documentclass{article}
\title{Normative Models Draft}
\author{Bonan Zhao}


% Text formats: margin, font, spacing
\usepackage[margin=0.8in]{geometry}
\usepackage{charter}
\renewcommand{\baselinestretch}{1.3}

% Shorthands
\newcommand{\featurespace}{F}
\newcommand{\colorspace}{\mathbb{CL}}
\newcommand{\shapespace}{\mathbb{SP}}

% Math stuff
\usepackage{amsmath}
\usepackage{amsfonts}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

% Settings for drawings
\usepackage{tikz,wrapfig}
\usetikzlibrary{shapes.geometric, arrows}

\begin{document}
\maketitle

\section{Task formalization}

A model of magic stones task consists of an agent $A$, a recipient $R$, and causal relation $\mathcal{C}$ representing the causal effects of agent $A$ on recipient $R$. We use $R'$ to denote recipient $R$ 's state after being affected by agent $A$. In sum, 
%
$$
\mathcal{C}(A, R) \models R \rightarrow R'$$

\subsection*{Feature space}

Consider a finite feature space $\featurespace$ consisting of only two kinds of features: colors $\colorspace$, and shape $\shapespace$. Color space contains three colors: $\colorspace = \{ c_1, c_2,  c_3 \}$, and shape space contains three shapes: $\shapespace = \{ s_1, s_2, s_3 \}$. The demo example can therefore be formalized as follows:
%
$$\mathcal{C}((c_1, s_3)^A, (c_2, s_1)^R) \models (c_2, s_1)^R \rightarrow (c_1, s_1)^{R'}
$$


\subsection*{Sample space}

The number of all possible events one may observe within such settings is a combination of all possible agents, all possible recipients, and all possible feature changes a recipient may exhibit. Hence, the number of all possible events is calculated by:
%
$$(|\colorspace| \times |\shapespace|)^3 = 729$$
	
These 729 possible events form the sample space for identifying causal relations. 

\section{Causal relations}

As the task explicitly instructed, some features of an agent cause some features of the recipient to change (or not change). 
%
Therefore, a causal relation $\mathcal{C}$ behaves as a function $f(O) = E$ where $O$ is a set of causes, and $E$ a set of effects. If an agent $a \in O$ and $f(O) = E$, then the effect should appear on the recipient.

\subsection*{Causes}

One may draw a set of causes based on different kinds of feature combinations:
%
(1) single features, like $c_1, c_2$, or $s_3$;
%
(2) feature combinations, like $c_1 \wedge s_3$, assuming both some colors \emph{and} some shapes cause the effect collectively;
%
(3) feature ranges, like $c_1 \vee c_2 \vee s_3$, assuming some features sharing the same causal effect;
%
(4) combination of feature ranges, for example $(c_1 \vee c_3) \wedge (s_1 \vee s_3)$, defining a delicate rule of causal effects.
%
Note that a special case for scenario (3), $c_1 \vee c_2 \vee c_3$, assumes that the same causal effect holds for all colors (similarly for $s_1 \vee s_2 \vee s_3$).

In the most generalized form, we can use a logical expression consisting of features, logical AND ($\wedge$), and logical OR ($\vee$) to construct possible sets of causes. Since we do not allow multiple colors (or shape) to appear on the same stone, a legit expression should be of a conjunctive normal form of maximal two clauses, where one clause stands for color features, and the other for shape features.

As a result, the number of all legit logical expressions for generating sets of causes is 
%
$$
	\sum_{i = 2}^{|\colorspace| + |\shapespace| -1} \binom{|\colorspace| + |\shapespace| }{i} +
 \left(
 	\sum_{j = 1}^{|\colorspace|} \binom{|\colorspace|}{j} \times
  	\sum_{k = 1}^{|\shapespace|} \binom{|\shapespace|}{k}
 \right)
	- 1
	= 104
$$

\subsection*{Effects}

Effects are easier to be modeled as pairs of feature changes: $c^R \rightarrow c^{R'}, s^R \rightarrow s^{R'}$.





Therefore, a causal relation $C$ is provided by a function 
%
$$f(\mathcal{O}(\featurespace)) = \mathcal{E}(\featurespace)$$
%
where $\featurespace$ is our feature space, $\mathcal{O}$ is an expression generating the cause feature set $C$, and $\mathcal{E}$ an expression generating the effect feature set $E$.


Since there are two expressions in a causal relation - one expression generating cause features, and another generating effect features, there are $110^2 = 12100$ possible causal relations in total in our current setting.


\section{Bayesian update}

% Figure for hierarchical Bayesian model framework
\tikzstyle{node} = [ellipse, minimum width=1.62cm, minimum height=0.8cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]
\begin{wrapfigure}{r}{0.25\textwidth}
\begin{center}
	\begin{tikzpicture}
		\node (ts) [node] {$T$};
		\node (hs) [node, below of=ts, yshift=-0.8cm] {$H$};
		\node (d) [node, below of=hs, yshift=-0.8cm] {$D$};
		\draw [arrow] (ts) -- (hs);
		\draw [arrow] (hs) -- (d);
	\end{tikzpicture}
	\caption{HBM}
	\label{framework}
\end{center}
\end{wrapfigure}

% Text begins here

Let's model participants' beliefs about the causal relations in a hierarchical Bayesian modeling (HBM) framework. Following a HBM framework, there are three levels of abstraction in our model: theory, hypothesis, and data, as shown in Figure~\ref{framework}.

At the highest level of abstraction, a theory space $T$ contains several candidate theories. Each theory $t \in T$ assigns a probability distribution over the hypothesis space $H$, giving each hypothesis $h \in H$ a weight accordingly. At the lowest level of abstraction is data space $D$ where a data point $d \in D$ is generated by some hypothesis, or observed by participants.

In our model, a theory $t$ can be an expression, or a set of expressions, that assigns some weight to the causal relations generated by such expressions. The complete set of all possible causal relations forms the hypothesis space $H$, and data is the observable features of agent stones and recipient stones.

\subsection*{Learning}

Given some data, theory learning follows standard Bayesian update rules. An agent's posterior belief about a theory $t \in T$ given data point $d$ is therefore calculated as in Equation~(\ref{post}).
	
	\vspace*{-2em}
	\begin{align*} \label{post}
		P(t|d) &\propto P(d|t) P(t) \numberthis \\
			   &\propto \sum_{h \in H} P(d|h) P(h|t) P(t)
	\end{align*}

where 
%
	$P(t)$ is a prior distribution over theories $t \in T$, 
	$P(h|t)$ is given by the definition of theory $t$, 
	and $P(d|h)$ is computed based on the definition of hypothesis $h$. 
%
Note that each theory $t \in T$ is a probability distribution over all hypotheses, therefore Equation~(\ref{post}) sums over all hypothesis $h \in H$.


\subsection*{Prediction}

When generalizing, agents make predictions on what data is likely to be generated given their posterior beliefs:

	\vspace*{-2em}
	\begin{align*} \label{pred}
		P(d') &= P(d'|H,T) \numberthis \\
			  &= \sum_{t \in T} \sum_{h \in H} P(d'|h, t) P(h|t) P(t)
	\end{align*}


\section{Model predictions}

Consider three cases: no theory, preference on parsimony, and preference on category.


\end{document}